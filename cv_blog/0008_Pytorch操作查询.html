<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- 上述3个meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ -->
    <title>Blog</title>
    <!-- Bootstrap core CSS -->
	<link href="../bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="../bootstrap/css/bootstrap-theme.min.css" rel="stylesheet">
    <!-- Bootstrap core js -->
    <script src="../jquery-3.4.1.min.js"></script>
    <script src="../bootstrap/js/bootstrap.min.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav class="navbar navbar-expand-sm bg-dark">
		<a class="navbar-brand text-light" href="../index.html">&nbsp&nbsp网站主页</a>
	</nav>
	<br>
    <div class="container">
        <div class="blog-header">
            <h1 class="blog-title">Pytorch基础操作</h1>
        </div>

        <div class="row">
            <div class="col-sm-8 blog-main">
                <p>
                    <div class="card">
                        <div class="card-header bg-info">
                            <h4>torch.cat</h4><a href="#" name="torch.cat"></a>
                        </div>
                        <div class="card-body">
                            可以同时拼接多个Tensor，但要确保这些Tensor<b>除了在拼接的维度上，其他维度都要相等</b>。例如：\([10,3,256,256]\)与\([15,3,256,256]\)在\(dim=0\)可以拼接，结果为\([25,3,256,256]\)；\([10,3,256,256]\)与\([10,5,256,256]\)在\(dim=1\)可以拼接，结果为\([10,8,256,256]\)。<br>
                            还可以拼接多个Tensor，对这些Tensor维度的要求与拼接2个时是一样的：
                            <pre>

>>> x = torch.randn(2, 3)
>>> x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
>>> torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
        -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
        -0.5790,  0.1497]])       
                            </pre>
                        </div>
                        <div class="card-header bg-info">
                            <h4>torch.squeeze与torch.unsqueeze</h4><a href="#" name="torch.squeeze"></a>
                        </div>
                        <div class="card-body">
                            压缩或扩张Tensor的维度。<br>
                            <code>torch.squeeze</code>：如果不加参数\(dim\)，则将所有等于1的维度删除；否则删除指定等于1的维度。
                            <pre>

>>> x = torch.zeros(2, 1, 2, 1, 2)
>>> x.size()
torch.Size([2, 1, 2, 1, 2])
>>> y = torch.squeeze(x)
>>> y.size()
torch.Size([2, 2, 2])
>>> y = torch.squeeze(x, 0)
>>> y.size()
torch.Size([2, 1, 2, 1, 2])
>>> y = torch.squeeze(x, 1)
>>> y.size()
torch.Size([2, 2, 1, 2])
                            </pre>
                            <code>torch.unsqueeze</code>：在指定\(dim\)插入一根轴。例如：\([3,256,256]\)在\(dim=1\)插入一根轴后变成\([1,3,256,256]\)。<b>这个操作常用在卷积神经网络推理单张图片时的预处理上面</b>。
                        </div>
                        <div class="card-header bg-info">
                            <h4>torch.view</h4><a href="#" name="torch.view"></a>
                        </div>
                        <div class="card-body">
                            更改一个<b>连续存储的Tensor</b>的Size。<code>torch.view</code>参数为输出Tensor的期望维度，允许有一维是-1，代表自动推理维度。
                            <pre>

>>> x = torch.randn(4, 4)
>>> x.size()
torch.Size([4, 4])
>>> y = x.view(16)
>>> y.size()
torch.Size([16])
>>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
>>> z.size()
torch.Size([2, 8])
                            </pre>
                            这里有一点值得注意，将view前后的Tensor同时flatten后，应该是会得到同一个Tensor的。这一点与transpose不同，因为<b>view是在不改变Tensor连续存储基础上进行的。</b>
                            <pre>

>>> a = torch.randn(1, 2, 3, 4)
>>> a.size()
torch.Size([1, 2, 3, 4])
>>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension
>>> b.size()
torch.Size([1, 3, 2, 4])
>>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory
>>> c.size()
torch.Size([1, 3, 2, 4])
>>> torch.equal(b, c)
False
                            </pre>
                        </div>
                        <div class="card-header bg-info">
                            <h4>torch.masked_select</h4><a href="#" name="torch.mask"></a>
                        </div>
                        <div class="card-body">
                            输入一个mask，取出Tensor中的部分数据。<b>输入的mask是一个2值的Tensor，不要求与Tensor的维度完全一致，只要能够广播维度就可以</b>。输出是一个一维的Tensor。一些目标检测算法在计算loss时，只在正样本锚框上面计算，mask用来筛选正样本十分方便。
                            <pre>

>>> x = torch.randn(3, 4)
>>> x
tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],
        [-1.2035,  1.2252,  0.5002,  0.6248],
        [ 0.1307, -2.0608,  0.1244,  2.0139]])
>>> mask = x.ge(0.5)
>>> mask
tensor([[False, False, False, False],
        [False, True, True, True],
        [False, False, False, True]])
>>> torch.masked_select(x, mask)
tensor([ 1.2252,  0.5002,  0.6248,  2.0139])
                            </pre>
                        </div>
                        <!-- <div class="card-header bg-info">
                            <h4></h4>
                        </div>
                        <div class="card-body">
                            
                        </div> -->
                    </div>
                </p>
            </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
            <div class="sidebar-module">
                <h4>友情链接</h4>
                <ol class="list-unstyled">
                    <li><a href="https://github.com/07hyx06">我的GitHub</a></li>
                    <li><a href="https://pytorch.org/docs/stable/index.html">Pytorch官方文档</a></li>
                </ol>
                <h4>联系我</h4>
                <p>1025723614@qq.com</p>
                <h4>快速导航</h4>
                <ol class="list-unstyled">
                    <li><a href="#torch.cat">torch.cat</a></li>
                    <li><a href="#torch.squeeze">torch.squeeze||unsqueeze</a></li>
                    <li><a href="#torch.view">torch.view</a></li>
                    <li><a href="#torch.mask">torch.masked_select</a></li>
                </ol>
            </div>
    </div>
</body>
</html>
